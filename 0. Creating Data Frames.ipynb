{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39dbfae",
   "metadata": {},
   "source": [
    "#### 66daysofdata\n",
    "### Day 1\n",
    "### pyspark tutorial\n",
    "1. Reading different formats in pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f0267a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60cc1ae",
   "metadata": {},
   "source": [
    "Py spark dataframes are implememnted on top of RDD\n",
    "wgen spark transformation starts, it does not immediatly start the transformation, just prepares for it, when commands like collect() are called it actually transforms\n",
    "\n",
    "pyspark applications start with initialization of the spark session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d990997",
   "metadata": {},
   "source": [
    "Data frames can be created from\n",
    "1. pyspark.sql.Rows\n",
    "2. lists\n",
    "3. tuples\n",
    "4. dictionaries\n",
    "5. pandas dataframe\n",
    "6. RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d57c531c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----------+-------------------+\n",
      "|  a|  b|    c|         d|                  e|\n",
      "+---+---+-----+----------+-------------------+\n",
      "|  1|0.1|row 1|2021-01-07|2021-01-07 22:00:00|\n",
      "|  2|0.2|row 2|2021-02-07|2021-02-07 22:00:00|\n",
      "+---+---+-----+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import datetime, date\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=0.1, c=\"row 1\", d=date(2021,1,7), e = datetime(2021,1,7,22,0,0)),\n",
    "    Row(a=2, b=0.2, c=\"row 2\", d=date(2021,2,7), e = datetime(2021,2,7,22,0,0))\n",
    "])\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df595094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----------+-------------------+\n",
      "| _1| _2|   _3|        _4|                 _5|\n",
      "+---+---+-----+----------+-------------------+\n",
      "|  1|0.1|row 1|2021-01-07|2021-01-07 22:00:00|\n",
      "|  2|0.2|row 2|2021-02-07|2021-02-07 22:00:00|\n",
      "+---+---+-----+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: double (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: date (nullable = true)\n",
      " |-- _5: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from tuples without schema\n",
    "df = spark.createDataFrame([\n",
    "    (1, 0.1, \"row 1\", date(2021,1,7), datetime(2021,1,7,22,0,0)),\n",
    "    (2, 0.2, \"row 2\", date(2021,2,7), datetime(2021,2,7,22,0,0))\n",
    "])\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "779f46b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----------+-------------------+\n",
      "|  a|  b|    c|         d|                  e|\n",
      "+---+---+-----+----------+-------------------+\n",
      "|  1|0.1|row 1|2021-01-07|2021-01-07 22:00:00|\n",
      "|  2|0.2|row 2|2021-02-07|2021-02-07 22:00:00|\n",
      "+---+---+-----+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from tuples with schema\n",
    "#couldn't create that empty f column with empty \n",
    "df = spark.createDataFrame([\n",
    "    (1, 0.1, \"row 1\", date(2021,1,7), datetime(2021,1,7,22,0,0)),\n",
    "    (2, 0.2, \"row 2\", date(2021,2,7), datetime(2021,2,7,22,0,0))\n",
    "],schema = 'a long, b double, c string, d date, e timestamp')\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7cb2d6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----------+-------------------+\n",
      "|  a|  b|    c|         d|                  e|\n",
      "+---+---+-----+----------+-------------------+\n",
      "|  1|2.0|row 1|2021-07-01|2021-07-01 12:00:00|\n",
      "|  2|3.0|row 2|2021-07-02|2021-07-02 12:00:00|\n",
      "+---+---+-----+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pandas dataframe\n",
    "import pandas as pd\n",
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2],\n",
    "    'b': [2., 3.],\n",
    "    'c': ['row 1', 'row 2'],\n",
    "    'd': [date(2021, 7, 1), date(2021, 7, 2)],\n",
    "    'e': [datetime(2021, 7, 1, 12, 0), datetime(2021, 7, 2, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0edfbec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----------+-------------------+\n",
      "|  a|  b|    c|         d|                  e|\n",
      "+---+---+-----+----------+-------------------+\n",
      "|  1|0.1|row 1|2021-01-07|2021-01-07 22:00:00|\n",
      "|  1|0.2|row 2|2021-02-07|2021-02-07 22:00:00|\n",
      "+---+---+-----+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from rdd\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    (1, 0.1, \"row 1\", date(2021,1,7), datetime(2021,1,7,22,0,0)),\n",
    "    (1, 0.2, \"row 2\", date(2021,2,7), datetime(2021,2,7,22,0,0))\n",
    "])\n",
    "df = spark.createDataFrame(rdd, schema=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f61ce36",
   "metadata": {},
   "source": [
    "TO DO\n",
    "### add empty values and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ffb59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
